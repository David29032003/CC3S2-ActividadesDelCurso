# Actividad Play with Kubertes

### Objetivos

- Comprender los conceptos b치sicos de contenedores como Docker y su orquestaci칩n a gran escala.
- Utilizar Kubernetes para desplegar y gestionar aplicaciones en m칰ltiples nodos de un cl칰ster.
- Aprender a dise침ar aplicaciones que escalen horizontalmente y manejen fallos de manera eficiente.
- Familiarizarse con pr치cticas de Infraestructura como C칩digo (IaC) para gestionar configuraciones de forma reproducible y automatizada.
- Integrar pr치cticas de DevOps para mejorar la colaboraci칩n y eficiencia en el desarrollo y operaci칩n de software.

Play with Kubernetes Classroom es una plataforma de Docker que permite obtener experiencia pr치ctica con Kubernetes de forma accesible y eficiente, directamente desde el navegador, sin necesidad de instalar software adicional. El taller en "Play with Kubernetes Classroom" gu칤a a los usuarios a trav칠s de m칩dulos que abarcan tanto los conceptos b치sicos como los avanzados de Kubernetes. Para una comprensi칩n completa de Kubernetes en esta actividad realizare el laboratorio de Kubernetes Hands-on Workshop. el cual se encuentra en la siguiente pagina: https://training.play-with-kubernetes.com/kubernetes-workshop/

En caso de que las salidas de los terminales sean demasiado extensas para capturarlas en pantalla, las mostrar칠 en formato de c칩digo en este README. Adem치s, muchos de los comandos del taller no funcionaron correctamente. He proporcionado la explicaci칩n de las salidas para cada comando, pero al inicio de la documentaci칩n asum칤 que los comandos funcionar칤an, lo cual result칩 no ser el caso. Por esta raz칩n, hay partes de la documentaci칩n que supone que los comandos funcionaron, ya que inicialmente se redact칩 bajo esa premisa antes de que tuviera tiempo para corregirlo.

En el taller de Kubernetes Hands-on Workshop encontrare las instrucciones y los terminales que debo utilizar:

![Descripcion](Imagenes/cap1.png)

### Introducci칩n

En este taller pr치ctico, aprendere los conceptos b치sicos de Kubernetes interactuando con Kubernetes a trav칠s de terminales de l칤nea de comandos. Finalmente, desplegare la aplicaci칩n de ejemplo Dockercoins en ambos nodos de trabajo.

### Primeros Pasos

쯈u칠 son estos terminales en el navegador?

A la derecha, deber칤a ver dos ventanas de terminal. Puede que me pidan iniciar sesi칩n primero, lo cual se puede hacer con un Docker ID o una cuenta de GitHub. Esos terminales son totalmente funcionales, ejecutando CentOS en la infraestructura de Play-with-Kubernetes. Cuando vea bloques de c칩digo, puedo hacer clic en ellos para que se autocompleten o copiarlos y pegarlos.

```bash
ls
```

![Descripcion](Imagenes/cap2.png)
![Descripcion](Imagenes/cap3.png)

### Iniciar el Cl칰ster

El primer paso es inicializar el cl칰ster en el primer terminal:

```bash
kubeadm init --apiserver-advertise-address $(hostname -i)
```

```sh
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.18:6443 --token nk8zwv.hxgo3xb7odfhtpwf \
        --discovery-token-ca-cert-hash sha256:68b75f61d5e5eaee2a58e237a57c5608c49ff0903b139cd22a2cadcf30ecf22c 
Waiting for api server to startup
Warning: resource daemonsets/kube-proxy is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
daemonset.apps/kube-proxy configured
No resources 
```

Explicacion:

- El comando `kubeadm init --apiserver-advertise-address $(hostname -i)` lo utilizo para inicializar un nodo maestro en un cl칰ster de Kubernetes, configurando la direcci칩n IP de la API del servidor. 

Despues copio toda la l칤nea que comienza con `kubeadm join` del primer terminal y lo pego en el segundo terminal. 

```bash
kubeadm join 192.168.0.18:6443 --token nk8zwv.hxgo3xb7odfhtpwf \
        --discovery-token-ca-cert-hash sha256:68b75f61d5e5eaee2a58e237a57c5608c49ff0903b139cd22a2cadcf30ecf22c 
```

Con lo cual vere algo como esto:

```sh
Initializing machine ID from random generator.
W0702 20:21:54.146831    4804 initconfiguration.go:120] Usage of CRI endpoints without URL scheme is deprecated and can cause kubelet errors in the future. Automatically prepending scheme "unix" to the "criSocket" with value "/run/docker/containerd/containerd.sock". Please update your configuration!
[preflight] Running pre-flight checks
        [WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
[preflight] The system verification failed. Printing the output from the verification:
KERNEL_VERSION: 4.4.0-210-generic
OS: Linux
CGROUPS_CPU: enabled
CGROUPS_CPUACCT: enabled
CGROUPS_CPUSET: enabled
CGROUPS_DEVICES: enabled
CGROUPS_FREEZER: enabled
CGROUPS_MEMORY: enabled
CGROUPS_PIDS: enabled
CGROUPS_HUGETLB: enabled
CGROUPS_BLKIO: enabled
        [WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "", err: exit status 1
        [WARNING FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

[node2 ~]$ 
```

Explicaion:

- Durante la inicializaci칩n, se genera una ID de m치quina aleatoria. Se emite una advertencia sobre el uso obsoleto de CRI endpoints sin un esquema de URL, que ahora se corrige autom치ticamente al agregar "unix" al valor "criSocket". Se realizan verificaciones previas al vuelo y se emite una advertencia sobre la habilitaci칩n del intercambio de memoria, lo cual debe desactivarse en entornos de producci칩n a menos que se pruebe la funci칩n NodeSwap del kubelet. Una vez completado, se informa que este nodo se ha unido al cl칰ster y se puede verificar su inclusi칩n ejecutando `kubectl get nodes` en el plano de control.

Eso significa que estoy casi listo. Por 칰ltimo, solo tengo que inicializar la red de mi cl칰ster en el primer terminal:

```bash
kubectl apply -n kube-system -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
```

Al ejecutar este comando en la terminal del nodo 1 obtengo la siguiente salida:

![Descripcion](Imagenes/cap4.png)

explicacion:

- La salida del comando `kubectl apply` intenta configurar Weave Net en Kubernetes, espec칤ficamente en el namespace `kube-system`. Sin embargo, el proceso falla debido a un problema de conexi칩n con el servidor de Kubernetes. El error "Unable to connect to the server" indica que Kubernetes no pudo resolver la direcci칩n `cloud.weave.works`, posiblemente debido a un fallo en la resoluci칩n DNS dentro del entorno de Kubernetes en el nodo `node1`. No pude resolver este problema pero de todas maneras explicare lo que deberia obtenerse al ejecutar ese comando proporcionado por el taller.

Deberia ver una salida como esta:

```bash
kubectl apply -n kube-system -f \
  >     "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 |tr -d '\n')"
  serviceaccount "weave-net" created
  clusterrole "weave-net" created
  clusterrolebinding "weave-net" created
  role "weave-net" created
  rolebinding "weave-net" created
  daemonset "weave-net" created
```

Explicacion:
- El comando `kubectl apply` se utiliza para aplicar la configuraci칩n de Weave Net en el namespace `kube-system`, obtenida din치micamente desde `https://cloud.weave.works/k8s/net`. Este URL incluye el par치metro `k8s-version`, generado codificando la versi칩n de Kubernetes en base64 con `kubectl version`. Los recursos necesarios para Weave Net, como serviceaccounts, clusterroles, clusterrolebindings, roles, rolebindings y un daemonset espec칤fico, han sido creados exitosamente. 

### 쯈u칠 es esta aplicaci칩n?

춰Es un minero de DockerCoin! 游눯游냡游닍游뚹

No, no puedo comprar caf칠 con DockerCoins.

C칩mo funciona DockerCoins:

- el trabajador solicita al rng que genere algunos bytes aleatorios

- el trabajador alimenta estos bytes al hasher
    y repite el proceso indefinidamente

- cada segundo, el trabajador actualiza redis para indicar cu치ntos bucles se han completado

- la interfaz web consulta redis, calcula y muestra la "velocidad de hash" en tu navegador

Explicacion: 

- DockerCoins funciona con un trabajador que pide al generador de n칰meros aleatorios (rng) que cree bytes aleatorios. Estos bytes son procesados por un hasher en un ciclo continuo. Cada segundo, el trabajador actualiza Redis con el n칰mero de ciclos completados. Luego, una interfaz web consulta Redis, calcula y muestra la velocidad de hash en el navegador del usuario.

### Obtener el c칩digo fuente de la aplicaci칩n

El taller tiene una aplicaci칩n de ejemplo para partes del taller. La aplicaci칩n est치 en el repositorio `dockercoins`.

![Descripcion](Imagenes/cap5.png)

Explicacion:
- El comando `git clone` se usa para clonar un repositorio Git desde la URL proporcionada. En este caso, se est치 clonando el repositorio `dockercoins` desde `https://github.com/dockersamples/dockercoins`. La salida muestra el progreso del clonado, incluyendo la enumeraci칩n de objetos, el conteo y compresi칩n de objetos, y finalmente la recepci칩n de los objetos con 칠xito. 

### Ejecutar la aplicaci칩n

Voy al directorio `dockercoins` en el repositorio clonado:

```bash
cd ~/dockercoins
```

Explicacion:

- El comando `cd ~/dockercoins` se utiliza para cambiar el directorio actual del usuario a `dockercoins`, que est치 ubicado en su directorio personal (`~`). La salida muestra que el directorio actual ha cambiado de `~/` (directorios personales) a `~/dockercoins/`, indicando que ahora est치s trabajando dentro del directorio `dockercoins` en el nodo `node1`.

Utilizo Compose para construir y ejecutar todos los contenedores:

```bash
docker-compose up
```

![Descripcion](Imagenes/cap7.png)

Explicacion: 
- Cuando intento ejecutar `docker-compose up`, surge un problema con el contenedor `worker_1`, el cual no puede resolver el nombre `hasher` para establecer una conexi칩n HTTP en el puerto 80. Este error apunta a dificultades en la resoluci칩n de nombres dentro de la red de contenedores. No logre resolver ese problema tampoco, voy a continuar el taller basandome netamente en lo que dice el taller con recpeto a la ejecucion de esta aplicacion.

Compose deberia decirle a Docker que construya todas las im치genes de contenedor (descargando las im치genes base correspondientes), luego inicia todos los contenedores y muestra los registros agregados.

### Generaci칩n de muchos registros

La aplicaci칩n deberia producir registros de forma continua. En estos registros, deberia ver c칩mo el servicio de trabajador hace solicitudes al generador de n칰meros aleatorios (rng) y al hasher. Para no saturar la vista principal, es 칰til poner estos procesos en segundo plano.

### Conexi칩n a la interfaz web

El contenedor de la interfaz web deberia ofrecer un panel de control accesible. Para visualizarlo, que se debe abrir un navegador web y conecta al nodo1 en el puerto 8000, que se cre칩 al ejecutar la aplicaci칩n.

### Limpieza

Antes de seguir, apaguemos todo presionando Ctrl-C.

![Descripcion](Imagenes/cap8.png)

### Conceptos de Kubernetes

Kubernetes es un sistema de gesti칩n de contenedores que ejecuta y administra aplicaciones en contenedores dentro de un cl칰ster. Esto significa que Kubernetes proporciona una plataforma robusta para escalar, gestionar y mantener aplicaciones distribuidas en entornos de producci칩n. Permite automatizar el despliegue, la escalabilidad y la gesti칩n de recursos de las aplicaciones, asegurando que funcionen de manera eficiente y confiable en todo momento.

### Cosas b치sicas que podemos pedirle a Kubernetes

Podemos instruir a Kubernetes para realizar tareas esenciales como iniciar 5 contenedores utilizando la imagen atseashop/api:v1.3, y colocar un balanceador de carga interno para gestionar el tr치fico hacia estos contenedores. Tambi칠n podemos iniciar 10 contenedores utilizando la imagen atseashop/webfront:v1.3, y configurar un balanceador de carga p칰blico para distribuir las solicitudes entrantes. En d칤as de alto tr치fico, como Black Friday o Navidad, podemos escalar nuestro cl칰ster agregando m치s contenedores. Adem치s, podemos actualizar a una nueva versi칩n reemplazando los contenedores existentes con la imagen atseashop/webfront:v1.4, asegurando que el procesamiento de solicitudes contin칰e sin interrupciones al actualizar los contenedores uno a uno.

### Otras capacidades de Kubernetes

Kubernetes ofrece una serie de capacidades adicionales que incluyen autoscaling b치sico para ajustar autom치ticamente la cantidad de recursos seg칰n la demanda. Tambi칠n permite despliegues blue/green y canarios para implementar nuevas versiones de aplicaciones de manera controlada y segura. Facilita la ejecuci칩n de servicios de larga duraci칩n y trabajos por lotes, as칤 como la sobrecomisi칩n del cl칰ster y el desalojo de trabajos de baja prioridad para optimizar el uso de recursos. Adem치s, Kubernetes es capaz de ejecutar servicios con datos persistentes, como bases de datos, y proporciona un control de acceso granular para gestionar qui칠n puede acceder y qu칠 pueden hacer en los recursos. Facilita la integraci칩n de servicios de terceros a trav칠s de un cat치logo de servicios y permite la automatizaci칩n de tareas complejas mediante operadores especializados.

### Arquitectura de Kubernetes

La arquitectura de Kubernetes se basa en una colecci칩n de servicios que constituyen su "cerebro": el servidor de API, que sirve como punto de entrada para todas las operaciones; servicios esenciales como el scheduler y el controlador; y etcd, un almac칠n de datos clave/valor altamente disponible que funciona como la "base de datos" de Kubernetes. Estos servicios se agrupan para formar lo que se conoce como el "master". Pueden ejecutarse directamente en un host o dentro de contenedores, con la opci칩n de ejecutar etcd en m치quinas separadas o en la misma infraestructura. Para garantizar alta disponibilidad, es posible configurar m치s de un master, aunque al menos uno es necesario para operaciones b치sicas.

### Arquitectura de Kubernetes: los nodos

Los nodos que ejecutan nuestros contenedores est치n equipados con una serie de servicios adicionales: un motor de contenedores (generalmente Docker), kubelet (el agente del nodo) y kube-proxy (un componente de red necesario pero no suficiente). Anteriormente, estos nodos sol칤an denominarse "minions". Es com칰n evitar ejecutar aplicaciones en el nodo o nodos que tambi칠n gestionan componentes del master, excepto en entornos de desarrollo con cl칰steres peque침os.

### Recursos de Kubernetes

La API de Kubernetes define una amplia variedad de objetos denominados recursos, los cuales se organizan por tipo o Kind en la API. Entre los tipos de recursos m치s comunes se encuentran: nodo (una m치quina f칤sica o virtual dentro del cl칰ster), pod (un grupo de contenedores que se ejecutan juntos en un nodo), servicio (un punto de acceso estable para conectarse a uno o varios contenedores), namespace (un grupo m치s o menos aislado de recursos), secret (un conjunto de datos sensibles destinados a ser pasados a un contenedor), y muchos otros m치s. La lista completa de recursos disponibles se puede consultar utilizando el comando `kubectl get`.

### Declarativo vs imperativo

En Kubernetes, hay una fuerte preferencia por el enfoque declarativo. En un enfoque declarativo, se especifica qu칠 se desea lograr sin detallar los pasos individuales para lograrlo. Por ejemplo, "Quiero una taza de t칠". En contraste, el enfoque imperativo detalla los pasos necesarios para alcanzar el objetivo, como "Hierve agua, vierte en una tetera, a침ade hojas de t칠, deja reposar y sirve en una taza". El enfoque declarativo puede parecer m치s sencillo siempre y cuando se conozcan los pasos necesarios para obtener el resultado deseado.

### 쯈u칠 ser칤a realmente declarativo?

En el contexto de Kubernetes, un enfoque realmente declarativo implica especificar el resultado deseado sin entrar en los detalles de c칩mo lograrlo paso a paso. Por ejemplo, "Quiero una taza de t칠, obtenida vertiendo una infusi칩n de hojas de t칠 en una taza. Una infusi칩n se obtiene dejando el objeto reposar unos minutos en agua caliente. El l칤quido caliente se obtiene verti칠ndolo en un recipiente adecuado y coloc치ndolo en una estufa". Este enfoque permite que Kubernetes gestione autom치ticamente los procesos necesarios para alcanzar el estado deseado, asegurando la coherencia y la eficiencia en el despliegue y gesti칩n de aplicaciones.

### Resumen de declarativo vs imperativo

En Kubernetes, los sistemas imperativos se caracterizan por su simplicidad y la necesidad de reiniciar desde cero si una tarea se interrumpe. En contraste, los sistemas declarativos permiten recuperarse de interrupciones o llegar a medio proceso al identificar lo que falta y ejecutar solo las acciones necesarias para alcanzar el estado deseado. Para ello, es crucial poder observar el sistema y calcular una "diferencia" entre el estado actual y el estado deseado, asegurando as칤 una administraci칩n eficiente y adaptable de los recursos de la aplicaci칩n en Kubernetes.

### Declarativo vs imperativo en Kubernetes

En Kubernetes, casi todo lo que creamos se define a partir de una especificaci칩n. Al trabajar con archivos YAML, buscamos los campos de especificaci칩n que describen c칩mo deseamos que sea el recurso. Kubernetes se encarga de reconciliar autom치ticamente el estado actual con la especificaci칩n mediante varios controladores. Cuando necesitamos modificar alg칰n recurso, actualizamos simplemente la especificaci칩n y Kubernetes se asegura de converger ese recurso seg칰n los nuevos par치metros definidos. Este enfoque permite una administraci칩n consistente y automatizada de los recursos en Kubernetes, facilitando la gesti칩n y escalabilidad de las aplicaciones en entornos de producci칩n.

### Modelo de red de Kubernetes

En Kubernetes, nuestro cl칰ster, compuesto por nodos y pods, opera como una gran red IP plana. Esto implica que todos los nodos deben poder comunicarse entre s칤 sin necesidad de NAT (traducci칩n de direcciones de red), al igual que todos los pods entre s칤 y los pods con los nodos. Cada pod es consciente de su propia direcci칩n IP sin requerir NAT. Kubernetes no especifica una implementaci칩n particular para la red, lo que permite flexibilidad en la configuraci칩n y optimizaci칩n de la red seg칰n las necesidades del entorno y las aplicaciones desplegadas.

### Modelo de red de Kubernetes: lo bueno

En el modelo de red de Kubernetes, todos los componentes pueden comunicarse directamente entre s칤 sin necesidad de traducci칩n de direcciones ni de puertos. Esto significa que los pods y los nodos pueden interactuar sin restricciones adicionales de protocolos. Sin embargo, es importante notar que los pods no pueden conservar su direcci칩n IP al moverse de un nodo a otro, y las direcciones IP no necesariamente tienen que ser "port치tiles" de un nodo a otro. La especificaci칩n del modelo de red es lo suficientemente simple como para permitir una variedad de implementaciones, lo que facilita la configuraci칩n y la optimizaci칩n de la red seg칰n los requisitos espec칤ficos del entorno y las aplicaciones desplegadas en Kubernetes.

### Modelo de red de Kubernetes: lo menos bueno

En el modelo de red de Kubernetes, si bien todos los componentes pueden comunicarse directamente entre s칤, se requieren pol칤ticas de red adicionales para garantizar la seguridad. Esto implica que la implementaci칩n de red utilizada debe ser compatible con estas pol칤ticas. Existen numerosas implementaciones disponibles (se mencionan 15 en la documentaci칩n de Kubernetes), lo que puede complicar la selecci칩n de la m치s adecuada. Aunque el modelo parece ofrecer una red de nivel 3, en realidad solo proporciona caracter칤sticas equivalentes a nivel 4, ya que la especificaci칩n requiere soporte para UDP y TCP pero no para rangos de puertos o paquetes IP arbitrarios. Adem치s, kube-proxy, al estar en la trayectoria de los datos al conectarse a pods o contenedores, puede afectar el rendimiento debido a su dependencia de t칠cnicas como proxying en userland o iptables.

### Modelo de red de Kubernetes: en pr치ctica

En la pr치ctica, los nodos que estamos utilizando han sido configurados para utilizar Weave como soluci칩n de red. No tenemos un respaldo particular hacia Weave, simplemente hemos encontrado que funciona bien para nuestras necesidades espec칤ficas. No es necesario preocuparse por la advertencia sobre el rendimiento de kube-proxy, a menos que este rutinariamente saturando interfaces de red de 10G, manejando millones de paquetes por segundo, operando plataformas de VOIP o juegos de alto tr치fico, o realizando operaciones peculiares que implican millones de conexiones simult치neas. En tales casos, es probable que ya este familiarizado con la configuraci칩n avanzada del kernel para optimizar el rendimiento de red en Kubernetes.

### Primer contacto con kubectl

Kubectl es la herramienta principal que utilizaremos para interactuar con Kubernetes. Se trata de una potente herramienta de l칤nea de comandos que facilita el acceso y la administraci칩n a trav칠s de la API de Kubernetes. Todo lo que se puede realizar con kubectl tambi칠n puede ejecutarse directamente a trav칠s de la API. Adem치s, puedo configurar kubectl utilizando el flag --kubeconfig para especificar un archivo de configuraci칩n, o directamente utilizando flags como --server, --user, entre otros. La pronunciaci칩n de kubectl puede variar, y com칰nmente se la puede escuchar como "Cube C T L", "Cube cuttle" o "Cube cuddle", seg칰n la preferencia del usuario.


### kubectl get

#### Visualizaci칩n de recursos de nodo

Utilizo `kubectl get` para explorar los recursos de nodo en el cl칰ster Kubernetes. 

![Descripcion](Imagenes/cap9.png)

Explicacion:
- Cuando ejecuto `kubectl get node`, obtengo una lista de nodos en el cl칰ster Kubernetes. En este caso, los nodos `node1` y `node2` est치n etiquetados como "NotReady", lo que indica que no est치n completamente disponibles para ejecutar cargas de trabajo. Ambos nodos tienen el rol de "control-plane" y est치n ejecutando la versi칩n `v1.27.2` de Kubernetes. 

Puedo obtener esta informaci칩n de varias maneras equivalentes:

- `kubectl get no`
- `kubectl get nodes`

![Descripcion](Imagenes/cap10.png)
![Descripcion](Imagenes/cap11.png)

#### Obtenci칩n de salida en formato legible por m치quina

`kubectl get` permite obtener resultados en formatos como JSON, YAML o formateados directamente:

- Para informaci칩n detallada de los nodos en formato ancho YAML:
  ```
  kubectl get nodes -o wide
  ```

![Descripcion](Imagenes/cap12.png)

Explicacion:
- El comando muestra una lista detallada de los nodos en el cl칰ster, incluyendo su estado, roles, edad, versi칩n de Kubernetes, direcciones IP internas y externas, imagen del sistema operativo, versi칩n del kernel y el runtime de contenedores. Se observa que ambos nodos `node1` y `node2` est치n en estado `NotReady`, con IPs internas `192.168.0.22` y `192.168.0.23`, ejecutando `CentOS Linux 7` y utilizando `containerd` como runtime de contenedores.

- Reviso la informacion de los nodos en el cluster en formato yaml:

![Descripcion](Imagenes/cap13.png)

```sh
[node1 ~]$ kubectl get no -o yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/docker/containerd/containerd.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2024-07-03T02:32:50Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: node1
      kubernetes.io/os: linux
      node-role.kubernetes.io/control-plane: ""
      node.kubernetes.io/exclude-from-external-load-balancers: ""
    name: node1
    resourceVersion: "800"
    uid: 3d2317d1-095a-475a-90e5-339fa0ecf42d
  spec:
    taints:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoSchedule
      key: node.kubernetes.io/not-ready
  status:
    addresses:
    - address: 192.168.0.22
      type: InternalIP
    - address: node1
      type: Hostname
    allocatable:
      cpu: "8"
      ephemeral-storage: 65504Mi
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 32946972Ki
      pods: "110"
    capacity:
      cpu: "8"
      ephemeral-storage: 65504Mi
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 32946972Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2024-07-03T02:37:46Z"
      lastTransitionTime: "2024-07-03T02:32:49Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2024-07-03T02:37:46Z"
      lastTransitionTime: "2024-07-03T02:32:49Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2024-07-03T02:37:46Z"
      lastTransitionTime: "2024-07-03T02:32:49Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2024-07-03T02:37:46Z"
      lastTransitionTime: "2024-07-03T02:32:49Z"
      message: 'container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady
        message:Network plugin returns error: cni plugin not initialized'
      reason: KubeletNotReady
      status: "False"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83
      - registry.k8s.io/etcd:3.5.7-0
      sizeBytes: 101639218
    - names:
      - registry.k8s.io/kube-apiserver@sha256:bbbc0eb287dbb7507948b1c05ac8f221d1a504e04572e61d4700ff18b2a3afd0
      - registry.k8s.io/kube-apiserver:v1.27.15
      sizeBytes: 34094789
    - names:
      - registry.k8s.io/kube-controller-manager@sha256:9ff408d91018df95a8505149e778bc7815b261ba8798497ae9319beb2b73304a
      - registry.k8s.io/kube-controller-manager:v1.27.15
      sizeBytes: 31536477
    - names:
      - registry.k8s.io/kube-proxy@sha256:23c54b01075318fe6991b224192faf6d65e9412b954b335efe326977deb30332
      - registry.k8s.io/kube-proxy:v1.27.15
      sizeBytes: 27495464
    - names:
      - registry.k8s.io/kube-scheduler@sha256:9a7746f46e126b23098a844b5e3df34ee44b14b666d540a1e92a21ca7bbaac99
      - registry.k8s.io/kube-scheduler:v1.27.15
      sizeBytes: 18124766
    - names:
      - registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097
      - registry.k8s.io/pause:3.9
      sizeBytes: 321520
    - names:
      - registry.k8s.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db
      - registry.k8s.io/pause:3.6
      sizeBytes: 301773
    nodeInfo:
      architecture: amd64
      bootID: ddf52b2b-0a45-4aa1-a34d-c9f704ef0f0f
      containerRuntimeVersion: containerd://1.6.21
      kernelVersion: 4.4.0-210-generic
      kubeProxyVersion: v1.27.2
      kubeletVersion: v1.27.2
      machineID: 535f290836d14b56adc84e2e6447d691
      operatingSystem: linux
      osImage: CentOS Linux 7 (Core)
      systemUUID: 6E717E48-761F-6640-AA8F-878704CE72ED
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      kubeadm.alpha.kubernetes.io/cri-socket: /run/docker/containerd/containerd.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2024-07-03T02:33:10Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: node2
      kubernetes.io/os: linux
    name: node2
    resourceVersion: "891"
    uid: 2485971c-5e95-4318-b0ee-12e1341c727b
  spec:
    taints:
    - effect: NoSchedule
      key: node.kubernetes.io/not-ready
  status:
    addresses:
    - address: 192.168.0.23
      type: InternalIP
    - address: node2
      type: Hostname
    allocatable:
      cpu: "8"
      ephemeral-storage: 65504Mi
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 32946972Ki
      pods: "110"
    capacity:
      cpu: "8"
      ephemeral-storage: 65504Mi
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 32946972Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2024-07-03T02:38:47Z"
      lastTransitionTime: "2024-07-03T02:33:10Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2024-07-03T02:38:47Z"
      lastTransitionTime: "2024-07-03T02:33:10Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2024-07-03T02:38:47Z"
      lastTransitionTime: "2024-07-03T02:33:10Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2024-07-03T02:38:47Z"
      lastTransitionTime: "2024-07-03T02:33:10Z"
      message: 'container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady
        message:Network plugin returns error: cni plugin not initialized'
      reason: KubeletNotReady
      status: "False"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - registry.k8s.io/kube-proxy@sha256:23c54b01075318fe6991b224192faf6d65e9412b954b335efe326977deb30332
      - registry.k8s.io/kube-proxy:v1.27.15
      sizeBytes: 27495464
    - names:
      - registry.k8s.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db
      - registry.k8s.io/pause:3.6
      sizeBytes: 301773
    nodeInfo:
      architecture: amd64
      bootID: ddf52b2b-0a45-4aa1-a34d-c9f704ef0f0f
      containerRuntimeVersion: containerd://1.6.21
      kernelVersion: 4.4.0-210-generic
      kubeProxyVersion: v1.27.2
      kubeletVersion: v1.27.2
      machineID: b3385e01068743c993f2d70f39c625a2
      operatingSystem: linux
      osImage: CentOS Linux 7 (Core)
      systemUUID: 6E717E48-761F-6640-AA8F-878704CE72ED
kind: List
metadata:
  resourceVersion: ""
[node1 ~]$ 
```

Explicacion:
- El comando `kubectl get no -o yaml` devuelve la informaci칩n detallada de los nodos en el cl칰ster en formato YAML. Este comando muestra todos los metadatos, anotaciones y configuraciones de cada nodo. Por ejemplo, en el resultado proporcionado, se muestra la versi칩n de la API, el tipo de recurso (Node), y las anotaciones espec칤ficas del nodo. 

#### Uso de jq con kubectl

Es posible generar informes personalizados utilizando `kubectl` en combinaci칩n con `jq`. Por ejemplo, para mostrar la capacidad de todos los nodos como objetos JSON:

```bash
kubectl get nodes -o json | jq ".items[] | {name:.metadata.name} + .status.capacity"
```

```sh
[node1 ~]$ kubectl get nodes -o json |
>       jq ".items[] | {name:.metadata.name} + .status.capacity"
{
  "name": "node1",
  "cpu": "8",
  "ephemeral-storage": "65504Mi",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "32946972Ki",
  "pods": "110"
}
{
  "name": "node2",
  "cpu": "8",
  "ephemeral-storage": "65504Mi",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "32946972Ki",
  "pods": "110"
}
[node1 ~]$ 
```

Explicacion:
- El comando `kubectl get nodes -o json | jq ".items[] | {name:.metadata.name} + .status.capacity"` obtiene informaci칩n en formato JSON sobre los nodos en el cl칰ster y la procesa con `jq` para extraer y mostrar el nombre del nodo junto con su capacidad. El resultado muestra que los nodos `node1` y `node2` tienen 8 CPUs, 65504Mi de almacenamiento ef칤mero, etc.

#### Exploraci칩n de recursos disponibles

`kubectl` ofrece facilidades de introspecci칩n para listar todos los tipos de recursos disponibles y ver detalles espec칤ficos de un recurso:

- Listar todos los tipos de recursos:
  ```
  kubectl get
  ```

- Ver detalles de un recurso:
  ```
  kubectl describe type/name
  ```
  o
  ```
  kubectl describe type name
  ```

- Ver la definici칩n de un tipo de recurso:
  ```
  kubectl explain type
  ```

#### Servicios en Kubernetes

Un servicio en Kubernetes es un punto de conexi칩n estable para conectar con algo. Para listar los servicios en nuestro cl칰ster:

```bash
kubectl get services
```

![Descripcion](Imagenes/cap14.png)

o simplemente
```bash
kubectl get svc
```

![Descripcion](Imagenes/cap15.png)

Explicacion:
- Ambos comandos muestran los servicios en el cl칰ster de Kubernetes. En este caso, hay un servicio llamado `kubernetes` de tipo `ClusterIP` con la IP `10.96.0.1`, sin una IP externa asignada, que est치 escuchando en el puerto 443/TCP y ha estado activo durante 20 minutos.

#### Servicios ClusterIP

Los servicios ClusterIP son internos y solo est치n disponibles dentro del cl칰ster, 칰tiles para la introspecci칩n desde contenedores:

- Intentar conectar al API con ClusterIP:
  ```bash
  curl -k https://10.96.0.1
  ```

![Descripcion](Imagenes/cap16.png)

Explicacion:
- El comando `curl -k https://10.96.0.1` intenta acceder al servicio de Kubernetes en la IP del cl칰ster. La respuesta es un mensaje de error JSON indicando que el acceso est치 prohibido. 

#### Listado de contenedores en ejecuci칩n

Los contenedores se manipulan a trav칠s de pods, que son grupos de contenedores que comparten recursos. Para listar los pods en nuestro cl칰ster:

```bash
kubectl get pods
```

![Descripcion](Imagenes/cap17.png)

Explicacion:
- El comando `kubectl get pods` muestra que no hay recursos, es decir, no hay pods actualmente en el namespace predeterminado de Kubernetes.

#### Namespaces

Los namespaces permiten la segregaci칩n de recursos. Para listar los namespaces en el cl칰ster:

```bash
kubectl get namespaces
```

![Descripcion](Imagenes/cap18.png)
o
```bash
kubectl get ns
```
![Descripcion](Imagenes/cap19.png)

Explicacion:
- Ambos comandos muestran una lista de namespaces en el cl칰ster de Kubernetes, indicando que los namespaces `default`, `kube-node-lease`, `kube-public` y `kube-system` est치n activos y tienen 25 minutos de existencia.

#### Acceso a namespaces

Tambien puedo cambiar al namespace deseado usando la opci칩n `-n`:

```bash
kubectl -n kube-system get pods
```

![Descripcion](Imagenes/cap20.png)

Explicacion:
- El comando muestra el estado de los pods en el namespace `kube-system`. Los pods `coredns-5d78c9869d-j2vmr` y `coredns-5d78c9869d-pfwcb` est치n en estado pendiente, mientras que otros pods esenciales como `etcd-node1`, `kube-apiserver-node1`, `kube-controller-manager-node1`, `kube-proxy-9dqgb`, `kube-proxy-gdsnw` y `kube-scheduler-node1` est치n en estado de ejecucion sin reinicios y tienen 26 minutos de actividad.

#### Componentes en los pods

Los pods incluyen varios componentes esenciales como `etcd`, `kube-apiserver`, `kube-controller-manager`, `kube-scheduler`, `kube-dns`, `kube-proxy`, y `weave`.

#### Ejecuci칩n de nuestro primer pod en Kubernetes

Para ejecutar un pod inicial y verificar los recursos creados:

```bash
kubectl run pingpong --image alpine ping 8.8.8.8
```

![Descripcion](Imagenes/cap21.png)

explicacion:

- El comando crea un pod llamado `pingpong` utilizando la imagen `alpine` y ejecuta el comando `ping` hacia la direcci칩n IP `8.8.8.8`. El pod `pingpong` fue creado exitosamente.

#### Detr치s de kubectl run

El comando `kubectl run` crea varios recursos como deployment (`deploy/pingpong`), replica set (`rs/pingpong-xxxx`), y pod (`po/pingpong-yyyy`).

Enumero la mayor칤a de los tipos de recursos:

```bash
kubectl get all
```

![Descripcion](Imagenes/cap22.png)

explicacion:
- El comando intenta mostrar los registros del deployment llamado `pingpong`, pero devuelve un error indicando que el deployment no fue encontrado en el cl칰ster.

#### 쯈u칠 son estas cosas diferentes?
Un `Deployment` en Kubernetes es un constructo de alto nivel que permite escalar, realizar actualizaciones continuas y retrocesos. Puede delegar la gesti칩n de pods a conjuntos de r칠plicas (`Replica Sets`). Por otro lado, un `Replica Set` es un constructo de bajo nivel que garantiza que un n칰mero espec칤fico de pods id칠nticos est칠 en ejecuci칩n y permite el escalado. El `Replication Controller` es su predecesor, aunque est치 obsoleto.

#### Nuestro despliegue pingpong
Cuando ejecutamos `kubectl run`, creamos un `Deployment` (`deploy/pingpong`). Este `Deployment` crea un `Replica Set` (`rs/pingpong-xxxx`), que a su vez crea un pod (`po/pingpong-yyyy`). Estos componentes trabajan juntos para escalar, mantener alta disponibilidad y realizar actualizaciones continuas.

#### Visualizaci칩n de la salida del contenedor
Para ver la salida de un contenedor en Kubernetes, use utiliza el comando `kubectl logs`. Podemos especificar el nombre de un pod o usar un selector como `deployment` o `replica set`. Por defecto, muestra los registros del primer contenedor en el pod.

Comando:
```
kubectl logs deploy/pingpong
```

![Descripcion](Imagenes/cap23.png)

explicacion:
- El comando intenta mostrar los registros del deployment llamado `pingpong`, pero devuelve un error indicando que el deployment no fue encontrado en el cl칰ster.

#### Transmisi칩n de registros en tiempo real
Similar a `docker logs`, `kubectl logs` admite opciones como `-f/--follow` para ver registros en tiempo real y `--tail` para ver un n칰mero espec칤fico de l칤neas desde el final. Podemos usar `kubectl logs --tail 1 --follow` para ver los registros m치s recientes en continuo.

```
kubectl logs deploy/pingpong --tail 1 --follow
```

![Descripcion](Imagenes/cap24.png)

Explicacion:
- El comando intenta mostrar los registros del deployment llamado `pingpong`, especificando que solo se muestre la 칰ltima l칤nea y que se sigan los registros en tiempo real. Sin embargo, devuelve un error indicando que el deployment no fue encontrado en el cl칰ster. 

#### Escalado de nuestra aplicaci칩n
Podemos escalar los contenedores (o pods) usando `kubectl scale`. Por ejemplo, `kubectl scale deploy/pingpong --replicas 8` aumentar칤a el n칰mero de r칠plicas del `Deployment` `pingpong`.

```
kubectl scale deploy/pingpong --replicas 8
```

![Descripcion](Imagenes/cap25.png)

Explicacion:
- El comando intenta escalar el deployment llamado `pingpong` a 8 r칠plicas, pero devuelve un error indicando que no se encontraron objetos v치lidos para escalar. 

#### Resiliencia
El `Deployment` `pingpong` supervisa su `Replica Set`, asegurando que el n칰mero correcto de pods est칠 en ejecuci칩n. Si un pod desaparece, el sistema lo detectar치 y lo reemplazar치 autom치ticamente.

```
kubectl get pods -w
```

![Descripcion](Imagenes/cap26.png)

Explicacion:

- El comando se utiliza para observar en tiempo real el estado de los pods en el cl칰ster Kubernetes. En este caso, muestra un pod llamado `pingpong` que est치 en estado "Pending", lo que significa que Kubernetes a칰n no ha podido programar este pod en ning칰n nodo del cl칰ster. 

```
kubectl delete pod pingpong-yyyy
```
![Descripcion](Imagenes/cap27.png)

Explicacion:
- El comando intenta eliminar un pod espec칤fico llamado `pingpong-yyyy`. Pero, el error "Error from server (NotFound): pods 'pingpong-yyyy' not found" indica que no se encontr칩 ning칰n pod con ese nombre en el cl칰ster Kubernetes. 

#### 쯈u칠 pasa si queremos algo diferente?
Si quiero ejecutar un contenedor de "un solo uso" que no se reinicie, podemos usar `kubectl run --restart=OnFailure` o `kubectl run --restart=Never`. Estos comandos crear칤an trabajos (`Jobs`) o pods en lugar de despliegues.

#### Visualizaci칩n de registros de m칰ltiples pods
Al especificar el nombre de un `Deployment`, `kubectl logs` muestra los registros de un 칰nico pod. Para ver registros de m칰ltiples pods, usamos un selector basado en etiquetas. Por ejemplo, `kubectl logs -l run=pingpong --tail 1` mostrar칤a la 칰ltima l칤nea de registros de todos los pods con la etiqueta `run=pingpong`.

```
kubectl logs -l run=pingpong --tail 1
```

![Descripcion](Imagenes/cap28.png)

Explicacion:
- El comando intenta mostrar los logs del 칰ltimo contenedor en ejecuci칩n que tenga la etiqueta `run=pingpong`. Pero, como no se muestra ning칰n resultado despu칠s de ejecutar el comando, esto podr칤a indicar que no hay pods actualmente en ejecuci칩n con esa etiqueta espec칤fica (`run=pingpong`). 

#### Limpieza
Para limpiar un despliegue, podemos eliminar con `kubectl delete deploy/pingpong`.

```
kubectl delete deploy/pingpong
```

![Descripcion](Imagenes/cap29.png)

Explicacion:
- El comando intenta eliminar el deployment llamado "pingpong", pero devuelve un error indicando que no se encontr칩 el deployment con ese nombre. Esto podr칤a ocurrir si el deployment "pingpong" no existe en el namespace actual o si lo especifique incorrectamente. 

#### Exponiendo contenedores
`kubectl expose` crea un servicio para los pods existentes en Kubernetes. Esto proporciona una direcci칩n estable para conectar a los pods. Puedo usar diferentes tipos de servicios como `ClusterIP`, `NodePort`, `LoadBalancer`, o `ExternalName` dependiendo de los requisitos de conectividad y acceso.

#### Ejecuci칩n de contenedores con puertos abiertos
Para ejecutar contenedores que necesitan puertos abiertos en Kubernetes, como los de ElasticSearch, utilizo `kubectl run` con la imagen espec칤fica y el n칰mero de r칠plicas. Puedo supervisar el estado de estos pods con `kubectl get pods -w`.

#### Exponiendo nuestro despliegue
Para exponer un despliegue, puedo crear un servicio `ClusterIP` predeterminado utilizando `kubectl expose`. Por ejemplo, puedo exponer el puerto de la API HTTP de ElasticSearch con `kubectl expose deploy/elastic --port 9200`.

```
kubectl expose deploy/elastic --port 9200
```

![Descripcion](Imagenes/cap30.png)

Explicacion:
- El comando intenta exponer el servicio asociado al deployment llamado "elastic", pero devuelve un error indicando que no se encontr칩 el deployment con ese nombre. 

direcci칩n IP que se debio asignar:

![Descripcion](Imagenes/cap31.png)

Explicacion:
- El comando muestra una lista de servicios desplegados en el cluster Kubernetes actual. En este caso, solo se muestra el servicio `kubernetes`, que es un servicio interno del cluster utilizado para la comunicaci칩n interna entre los componentes de Kubernetes. 

#### Los servicios son constructos de capa 4
Puedo asignar direcciones IP a los servicios, pero siguen siendo de capa 4 (es decir, un servicio no es solo una direcci칩n IP; es una direcci칩n IP + protocolo + puerto). Esto se debe a la implementaci칩n actual de kube-proxy, que depende de mecanismos que no admiten la capa 3. Por lo tanto, debo indicar el n칰mero de puerto para mi servicio.

#### Probando nuestro servicio
Voy a enviar algunas solicitudes HTTP a los pods de ElasticSearch:

```bash
IP=$(kubectl get svc elastic -o go-template --template '{{ .spec.clusterIP }}')
```

![Descripcion](Imagenes/cap32.png)

Explicacion:
- El comando intenta obtener detalles del servicio llamado "elastic" en el cluster Kubernetes, pero devuelve un error indicando que el servicio no fue encontrado. Esto significa que no existe un servicio llamado "elastic" actualmente desplegado en el namespace o contexto de Kubernetes desde donde se ejecut칩 el comando. 

Trato de enviar algunas solicitudes:

```bash
curl http://$IP:9200/
```

![Descripcion](Imagenes/cap33.png)

Explicacion:
- El comando intenta hacer una solicitud HTTP al IP especificado en el entorno de Kubernetes, pero devuelve un error indicando que no se pudo resolver el host. 

#### Limpieza
Termino con el despliegue de ElasticSearch, as칤 que voy a limpiarlo:

```bash
kubectl delete deploy/elastic
```

![Descripcion](Imagenes/cap34.png)

Explicacion:
- El comando intenta eliminar el deployment llamado "elastic", pero devuelve un error indicando que no se encontr칩 ning칰n deployment con ese nombre. 

#### Nuestra aplicaci칩n en Kubernetes
쯈u칠 se tiene planeado?

En esta parte, voy a:

- Construir im치genes para la aplicaci칩n,
- Publicar estas im치genes en un registro,
- Ejecutar despliegues utilizando estas im치genes,
- Exponer estos despliegues para que puedan comunicarse entre s칤,
- Exponer la interfaz web para acceder desde fuera.

#### El plan
1. Construir en nuestro nodo de control (node1),
2. Etiquetar las im치genes para que se nombren como $USERNAME/servicename,
3. Subirlas a Docker Hub,
4. Crear despliegues utilizando las im치genes,
5. Exponer (con un ClusterIP) los servicios que necesitan comunicarse,
6. Exponer (con un NodePort) la interfaz web.

#### Configuraci칩n
En el primer terminal, establecere una variable de entorno para mi nombre de usuario de Docker Hub. Debo asegurarme de estar en el directorio `dockercoins`.

```bash
export USERNAME=TU_USUARIO
pwd
```

![Descripcion](Imagenes/cap35.png)

Explicacion:
- El comando establece una variable de entorno llamada `USERNAME` con el valor `davidreevesgoni`. La variable `USERNAME` estar치 disponible para cualquier comando o script que se ejecute en esa sesi칩n de terminal, permitiendo que el valor `davidreevesgoni` sea utilizado como referencia en diversas operaciones dentro del entorno de trabajo actual.

![Descripcion](Imagenes/cap36.png)

Explicacion:
- El comando `pwd` muestra la ruta completa del directorio actual en el cual se encuentra el usuario actualmente. En este caso, el resultado indica que el directorio actual es `/root`, que es el directorio ra칤z del usuario `root`. 

#### Una nota sobre los registros
Para este taller, usare Docker Hub. Tambi칠n hay otras opciones, como Docker Trusted Registry y Docker Open Source Registry.

#### Docker Hub
Docker Hub es el registro predeterminado para Docker. Los nombres de imagen en Hub son simplemente `$USERNAME/$IMAGENAME` o `$ORGANIZATIONNAME/$IMAGENAME`.

Para usar Hub, debo asegurarme de tener una cuenta. Luego, escribere `docker login` en el terminal e inicio sesi칩n con mi nombre de usuario y contrase침a.

![Descripcion](Imagenes/cap67.png)

Explicacion:
- Al ejecutar el comando `docker login`, inici칠 sesi칩n con mi Docker ID para facilitar la descarga y subida de im치genes desde Docker Hub. Ingres칠 mi nombre de usuario y contrase침a cuando se solicit칩, y Docker valid칩 estas credenciales. 

#### Construcci칩n y subida de nuestras im치genes
Utilizare una funci칩n conveniente de Docker Compose. Voy al directorio `stacks`,  construyo y subo las im치genes:

```bash
cd ~/dockercoins/stacks
docker-compose -f dockercoins.yml build
docker-compose -f dockercoins.yml push
```

![Descripcion](Imagenes/cap37.png)
![Descripcion](Imagenes/cap38.png)

Explicacion:
- Cuando se ejecuta estos comandos en el directorio actual (`./`), no lo encuentra y muestra un error de tipo `FileNotFoundError`. Esto indica que el archivo YAML necesario para la configuraci칩n de Docker Compose no est치 presente en el directorio desde donde se est치 ejecutando el comando. 

#### Despliegue de todas las cosas
Ahora trato de desplegar el c칩digo (as칤 como una instancia de redis). Despleguo redis y todo lo dem치s:

```bash
kubectl run redis --image=redis

for SERVICE in hasher rng webui worker; do
  kubectl run $SERVICE --image=$USERNAME/$SERVICE -l app=$SERVICE
done
```

![Descripcion](Imagenes/cap39.png)

Explicacion:
- Cuando se ejecuta el comando, se crea un nuevo pod en Kubernetes utilizando la imagen de Redis desde el registro de contenedores predeterminado. 

![Descripcion](Imagenes/cap40.png)

Explicacion:
- Al ejecutar estos comandos, se est치n creando varios pods en Kubernetes de manera automatizada. 

#### 쮼st치 funcionando?
Despu칠s de esperar a que se complete el despliegue, reviso los registros. Para ello utilizo `kubectl get deploy -w` para ver eventos de despliegue y `kubectl logs` para ver los registros de rng y worker.

![Descripcion](Imagenes/cap41.png)

Explicacion:
- Al intentar obtener los logs de `deploy/rng` y `deploy/worker` utilizando el comando `kubectl logs`, se genera un error indicando que no se encontraron los deployments especificados. 

#### Exponiendo servicios
#### Internamente
Expongo cada despliegue, especificando el puerto correcto:

```bash
kubectl expose deployment redis --port 6379
kubectl expose deployment rng --port 80
kubectl expose deployment hasher --port 80
```

![Descripcion](Imagenes/cap42.png)

Explicacion:
- Al intentar exponer los deployments `redis`, `rng`, y `hasher` utilizando el comando `kubectl expose deployment`, se genera un error indicando que no se encontraron los deployments especificados. 

#### 쮼st치 funcionando ya?
El worker tiene un bucle infinito que se reintentar치 cada 10 segundos despu칠s de un error. Transmite los registros del worker para verlo funcionar:

```bash
kubectl logs deploy/worker --follow
```

![Descripcion](Imagenes/cap43.png)

Explicacion:
- Cuando intento ver los logs del deployment llamado `worker` utilizando `kubectl logs deploy/worker --follow`, recibo un error que indica que no se encontr칩 el deployment con ese nombre en el cl칰ster de Kubernetes. 

#### Para acceso externo
Ahora quiero acceder a la interfaz web. La expondre con un NodePort. Creo un servicio NodePort para la interfaz web:

```bash
kubectl create service nodeport webui --tcp=80 --node-port=30001
```

![Descripcion](Imagenes/cap44.png)

Explicacion:
- Cuando ejecuto el comando, estoy creando un nuevo servicio llamado `webui` de tipo NodePort en Kubernetes. Este servicio expone el puerto 80 de los pods seleccionados por el selector predeterminado, permitiendo acceder a ellos desde fuera del cl칰ster a trav칠s del puerto 30001 en cada nodo del cl칰ster. 

Verificar que el puerto que fue asignado:

![Descripcion](Imagenes/cap45.png)

#### Acceso a la interfaz web
Ahora se deberia conectar cualquier nodo al puerto asignado para ver la interfaz web.

#### Implicaciones de seguridad de kubectl apply

Cuando utilizo `kubectl apply -f <URL>`, creo recursos arbitrarios. Los recursos pueden ser maliciosos; por ejemplo un despliegue que:

- Inicie mineros de Bitcoin en todo el cl칰ster
- Se esconda en un espacio de nombres no predeterminado
- Monte enlazado el sistema de archivos de nuestros nodos
- Inserte claves SSH en la cuenta ra칤z (en el nodo)
- Encripte nuestros datos y los rinda

`kubectl apply` es el nuevo `curl | sh`. `curl | sh` es conveniente y seguro si se utilizan URLs HTTPS de fuentes confiables. `kubectl apply -f` tambi칠n es conveniente y seguro bajo las mismas condiciones, pero introduce nuevos modos de fallo.

#### Escalando un despliegue

Comenzare con uno f치cil: el despliegue del worker.

```bash
kubectl get pods
kubectl get deployments
kubectl scale deploy/worker --replicas=10
```

![Descripcion](Imagenes/cap46.png)

![Descripcion](Imagenes/cap47.png)

Explicacion:
- Al ejecutar el comando, se indica que no se encontraron recursos en el namespace por defecto (`default`), lo que sugiere que no hay despliegues (deployments) activos en este momento en ese namespace.
- Al intentar escalar el deployment llamado `worker` a 10 r칠plicas utilizando el comando `kubectl scale`, se devuelve un error que indica "no objects passed to scale". Esto significa que el comando no pudo encontrar el deployment llamado `worker` en el cl칰ster Kubernetes. 

Despu칠s de unos segundos, el gr치fico en la interfaz web deber칤a aparecer (y alcanzar hasta 10 hashes/segundo, igual que cuando estaba ejecutando en uno solo).

#### Daemon sets

쯈u칠 pasa si quiero una (y exactamente una) instancia de `rng` por nodo? Si simplemente escalo `deploy/rng` a 2, nada garantiza que se propaguen. En lugar de un despliegue, utilizare un daemon set.

#### Creando un daemon set

Desafortunadamente, a partir de Kubernetes 1.9, la CLI no puede crear daemon sets. Sin embargo, cualquier tipo de recurso siempre se puede crear proporcionando una descripci칩n YAML:

```bash
kubectl apply -f foo.yaml
```

#### Creando el archivo YAML para nuestro daemon set

Comienzo con el archivo YAML para el recurso `rng` actual.

```bash
kubectl get deploy/rng -o yaml --export > rng.yml
```

![Descripcion](Imagenes/cap48.png)

Explicacion:
- El comando intenta exportar la configuraci칩n del deployment llamado `rng` a un archivo YAML llamado `rng.yml`. Sin embargo, el error "unknown flag: --export" indica que la versi칩n de `kubectl` utilizada no admite el flag `--export` en la operaci칩n `kubectl get`.

#### "Lanzar" un recurso a otro

쯈u칠 pasa si simplemente cambio el campo `kind`?

```bash
vi rng.yml
```

Cambio `kind: Deployment` a `kind: DaemonSet`, guardo y salgo.

Intento crear mi nuevo recurso:

```bash
kubectl apply -f rng.yml
```

![Descripcion](Imagenes/cap66.png)

Explicacion:
- El comando intenta aplicar los recursos definidos en el archivo YAML rng.yml, pero devuelve un error indicando que no se encontraron objetos para aplicar.

#### Usando el --force, Luke

Puedo tratar de decirle Kubernetes que ignore estos errores y lo intente de todos modos.

```bash
kubectl apply -f rng.yml --validate=false
```

![Descripcion](Imagenes/cap49.png)

Explicacion:
- El comando intenta aplicar la configuraci칩n contenida en el archivo `rng.yml` al cluster Kubernetes, deshabilitando la validaci칩n de los recursos. El error "no objects passed to apply" indica que el archivo `rng.yml` no contiene ning칰n objeto Kubernetes v치lido para aplicar.

#### Comprobando lo que hemos hecho

쯊ransforme el despliegue en un daemon set?

```bash
kubectl get all
```

![Descripcion](Imagenes/cap50.png)

Explicacion:
- El comando muestra el estado actual de todos los recursos desplegados en el cl칰ster Kubernetes. En la salida proporcionada, se observa que varios pods (`hasher`, `pingpong`, `redis`, `rng`, `webui`, `worker`) est치n en estado "Pending", lo que indica que a칰n no se han programado en ning칰n nodo del cl칰ster para su ejecuci칩n. 

#### Explicaci칩n

Puedo tener diferentes tipos de recursos con el mismo nombre (es decir, un despliegue y un daemon set ambos nombrados `rng`). Todav칤a tengo el antiguo despliegue `rng`, pero ahora tambi칠n tengo el nuevo daemon set `rng`.

#### 쯈u칠 est치n haciendo todos estos pods?

Verifico los registros de todos estos pods `rng`. Todos estos pods tienen una etiqueta `run=rng`.

```bash
kubectl logs -l run=rng --tail 1
```

![Descripcion](Imagenes/cap51.png)

Explicacion:
- El comando intenta mostrar los registros del 칰ltimo contenedor que ejecutaba la etiqueta `run=rng`. Sin embargo, en este caso, no se encontraron recursos en el espacio de nombres predeterminado que coincidan con esa etiqueta, lo que sugiere que no hay ning칰n pod actualmente desplegado que tenga esa etiqueta espec칤fica.

#### Removiendo el primer pod del balanceador de carga

쯈u칠 suceder칤a si elimino ese pod con `kubectl delete pod ...`?

- El replicaset lo recrear칤a inmediatamente.

쯈u칠 pasar칤a si elimino la etiqueta `run=rng` de ese pod?

- El replicaset lo recrear칤a inmediatamente.

Porque lo que importa para el replicaset es el n칰mero de pods que coinciden con ese selector.

#### Profundizando en los selectores

Voy a ver los selectores para el despliegue `rng` y el replicaset asociado.

Muestro informaci칩n detallada sobre el despliegue `rng`:

```bash
kubectl describe deploy rng
```

![Descripcion](Imagenes/cap52.png)

Explicacion:
- El comando intenta describir los detalles del despliegue llamado `rng`. Pero presenta un error un error que indica que no se pudo encontrar ning칰n despliegue con el nombre "rng" en el cl칰ster. 

Muestro informaci칩n detallada sobre el replicaset `rng-yyyy`:

```bash
kubectl describe rs rng-yyyy
```

![Descripcion](Imagenes/cap53.png)

Explicacion:
- El comando intenta describir los detalles de un conjunto de r칠plicas con el nombre "rng-yyyy". Pero presenta un error que indica que no se pudo encontrar ning칰n conjunto de r칠plicas con ese nombre en el cl칰ster. 

El selector del replicaset tambi칠n debe tener un `pod-template-hash`, a diferencia de los pods en mi daemon set.

#### Actualizando un servicio a trav칠s de etiquetas y selectores

쯈u칠 pasa si quiero quitar el despliegue `rng` del balanceador de carga?

Opci칩n 1:
- Destruirlo.

Opci칩n 2:
- Agregar una etiqueta adicional al daemon set
- Actualizar el selector del servicio para que se refiera a esa etiqueta.

#### Agregar una etiqueta adicional al daemon set

Voy a actualizar la especificaci칩n del daemon set.

Opci칩n 1:
- Editar el archivo `rng.yml` que usamos antes.
- Cargar la nueva definici칩n con `kubectl apply`.

Opci칩n 2:
- Usar `kubectl edit`.

Actualizo el daemon set para agregar `isactive: "yes"` a la etiqueta del selector y de la plantilla:

```bash
kubectl edit daemonset rng
```

![Descripcion](Imagenes/cap54.png)

Explicacion:
- El comando intenta abrir y editar el daemonset llamado "rng". Pero presenta un error indicando que no se pudo encontrar ning칰n daemonset con ese nombre en el cl칰ster actual. 

```bash
kubectl edit service rng
```

![Descripcion](Imagenes/cap55.png)

Explicacion:
- El comando intenta abrir y editar el servicio llamado "rng", pero el error indica que no se pudo encontrar ning칰n servicio con ese nombre en el cl칰ster actual. 

#### Verificando lo que hemos hecho

Verifico los registros de todos los pods `run=rng` para confirmar que solo 2 de ellos est치n activos ahora:

```bash
kubectl logs -l run=rng
```

![Descripcion](Imagenes/cap56.png)

Explicacion:
- El comando intenta mostrar los registros de los pods que tienen la etiqueta `run=rng`, pero no se encontraron recursos que coincidan con esa etiqueta en el espacio de nombres predeterminado. 

Los sellos de tiempo deber칤an darme una pista sobre cu치ntos pods est치n recibiendo tr치fico actualmente.

Miro los pods que tengo ahora:

```bash
kubectl get pods
```

![Descripcion](Imagenes/cap57.png)

#### Recuper치ndonos de un despliegue fallido

Podria empujar alguna imagen v0.3 (la l칩gica de reintentos del pod eventualmente lo capturar치 y el despliegue continuar치).

O podria invocar un rollback manual.

Cancelar el despliegue y esperar a que el polvo se asiente:

```bash
kubectl rollout undo deploy worker
kubectl rollout status deploy worker
```

![Descripcion](Imagenes/cap65.png)

Explicacion:
- El comando intenta revertir un despliegue anterior del deployment llamado "worker", pero devuelve un error indicando que no se encontr칩 el deployment especificado.

#### Cambiar par치metros de rollout

Queremos:
- Revertir a v0.1.
- Ser conservadores en la disponibilidad (siempre tener el n칰mero deseado de workers disponibles).
- Ser agresivos en la velocidad de rollout (actualizar m치s de un pod a la vez).
- Dar algo de tiempo a los workers para "calentarse" antes de comenzar m치s.

#### Aplicar cambios a trav칠s de un parche YAML

Podria usar `kubectl edit deployment worker`, pero tambi칠n podria usar `kubectl patch` con el YAML exacto mostrado antes.

Aplicar todos mis cambios y esperar a que surtan efecto:

![Descripcion](Imagenes/cap66.png)

Explicacion:
- El comando est치 intentando aplicar los recursos definidos en el archivo rng.yml, pero el error indica que no se encontraron objetos v치lidos para aplicar en el archivo especificado.

#### Pr칩ximos pasos

Bien, 쯖칩mo empiezo a containerizar mis aplicaciones?

Lista de verificaci칩n sugerida para la containerizaci칩n:

- Escribir un Dockerfile para un servicio en una aplicaci칩n.
- Escribir Dockerfiles para los otros servicios (compilables).
- Escribir un archivo Compose para toda esa aplicaci칩n.
- Asegurarse de que los desarrolladores puedan ejecutar la aplicaci칩n en contenedores.
- Configurar construcciones autom치ticas de im치genes de contenedores desde el repositorio de c칩digo.
- Configurar un pipeline de integraci칩n continua utilizando estas im치genes.
- Configurar un pipeline de entrega continua (para staging/QA) utilizando estas im치genes.

춰Y luego es hora de mirar la orquestaci칩n!

#### Espacios de nombres (Namespaces)

Los espacios de nombres me permiten ejecutar m칰ltiples pilas id칠nticas lado a lado.

Dos espacios de nombres (por ejemplo, azul y verde) pueden tener su propio servicio redis.

Cada uno de los dos servicios redis tiene su propio ClusterIP.

kube-dns crea dos entradas, mapeando estas dos direcciones IP de ClusterIP:

- `redis.blue.svc.cluster.local` y `redis.green.svc.cluster.local`

Los pods en el espacio de nombres azul obtienen un sufijo de b칰squeda de `blue.svc.cluster.local`.

Como resultado, resolver redis desde un pod en el espacio de nombres azul devuelve el redis "local".

Esto no proporciona aislamiento. Esa ser칤a la tarea de las pol칤ticas de red.

#### Servicios persistentes (databases, etc.)

Como primer paso, es m치s prudente mantener los servicios persistentes fuera del cl칰ster.

Exponerlos a los pods se puede hacer con m칰ltiples soluciones:

- Servicios ExternalName (redis.blue.svc.cluster.local ser치 un registro CNAME).
- Servicios ClusterIP con Endpoints expl칤citos (en lugar de dejar que Kubernetes genere los endpoints a partir de un selector).
- Servicios Ambassador (proxies a nivel de aplicaci칩n que pueden proporcionar inyecci칩n de credenciales y m치s).

#### Manejo del tr치fico HTTP

Los servicios son construcciones de capa 4.

HTTP es un protocolo de capa 7.

Es manejado por ingresses (un recurso diferente).

Los ingresses permiten:

- Enrutamiento virtual de hosts.
- Persistencia de sesi칩n.
- Mapeo de URI.
- 춰Y mucho m치s!

#### Registro y m칠tricas

El registro se delega al motor de contenedores.

Las m칠tricas t칤picamente se manejan con Prometheus.

#### Manejo de la configuraci칩n de nuestras aplicaciones

Dos construcciones son particularmente 칰tiles: secrets y config maps.

Permiten exponer informaci칩n arbitraria a nuestros contenedores.

Evita almacenar la configuraci칩n en im치genes de contenedores (hay algunas excepciones a esa regla, pero generalmente es una mala idea).

Nunca se debe almacenar informaci칩n sensible en im치genes de contenedores (es el equivalente a tener la contrase침a en una nota adhesiva en la pantalla).

#### Federaci칩n de cl칰steres

La operaci칩n maestra de Kubernetes depende de etcd.

etcd utiliza el protocolo Raft.

Raft recomienda baja latencia entre nodos.

쯈u칠 pasa si el cl칰ster se extiende a m칰ltiples regiones?

Se debe dividir en cl칰steres locales.

Agr칰palos en una federaci칩n de cl칰steres.

Sincronizar recursos entre cl칰steres.

Descubrir recursos entre cl칰steres.

#### Experiencia del desarrollador
쮺칩mo integras a un nuevo desarrollador?
- 쯈u칠 necesitan instalar para obtener una pila de desarrollo?
- 쮺칩mo se hace que un cambio de c칩digo pase de dev a producci칩n?
- 쮺칩mo alguien a침ade un componente a una pila?

1. **Integraci칩n de un nuevo desarrollador:**
   Al integrar a un nuevo desarrollador en el equipo, es fundamental que instale Docker y configure Kubernetes localmente para ejecutar la pila de desarrollo de manera consistente. Esto asegura que todos los miembros del equipo trabajen en un entorno reproducible y coherente. Adem치s, familiarizarse con los Dockerfiles y los archivos Compose es crucial para comprender la estructura y la interacci칩n de los servicios dentro del entorno Kubernetes.

2. **Instalaci칩n para obtener una pila de desarrollo:**
   Para obtener una pila de desarrollo funcional, los nuevos desarrolladores deben instalar varias herramientas esenciales. Esto incluye configurar Docker en sus m치quinas para gestionar contenedores que alojen cada servicio de la aplicaci칩n. Adem치s, es crucial configurar un entorno local de Kubernetes usando herramientas como `minikube` o `Docker Desktop`, junto con `kubectl` para interactuar con el cl칰ster Kubernetes local. Esta configuraci칩n permite a los desarrolladores ejecutar y probar la aplicaci칩n en un entorno similar al de producci칩n, facilitando el desarrollo y la depuraci칩n de problemas antes de desplegar el c칩digo en entornos cr칤ticos.

3. **Cambio de c칩digo de dev a producci칩n:**
   Los cambios de c칩digo se mueven de dev a producci칩n a trav칠s de un pipeline de CI bien definido. Durante este proceso, el c칩digo se compila en contenedores Docker y se somete a pruebas automatizadas para validar su funcionalidad. Despu칠s de pasar las pruebas en entornos de staging o QA, el pipeline de CD despliega autom치ticamente los contenedores probados en producci칩n. Este enfoque controlado y automatizado minimiza el riesgo de fallos en producci칩n al tiempo que garantiza que las nuevas versiones se desplieguen de manera segura y eficiente.

4. **Adici칩n de un componente a una pila:**
   A침adir un nuevo componente a la pila implica definir un Dockerfile para el servicio adicional y actualizar el archivo Compose correspondiente. Estos cambios se incorporan al pipeline de CI/CD, donde el nuevo componente se construye, prueba y despliega autom치ticamente en los entornos de desarrollo y producci칩n. Este proceso asegura que la integraci칩n del nuevo componente sea coherente y eficiente, manteniendo la estabilidad del sistema mientras se escala la infraestructura seg칰n sea necesario.
